# Apache Spark Assignment

Author: [Riyaz Nakarmi]
Email: [riyaz@fusemachines.com]

## Table of Contents
- [Day 1](#day-1)
- [Day 2](#day-2)
- [Day 3](#day-3)
- [Day 4](#day-4)

## Day 1
- *Task 1: What is Apache Spark.*
  - Subtask 1.1: Runnig Spark
  - Subtask 1.2: Downloading Spark Locally
- *Task 2: Gentle Introduction to spark*
  - Subtask 2.1: Spark’s Basic Architecture
  - Subtask 2.2: The SparkSession
  - Subtask 2.3: DataFrames
  - Subtask 2.4: Partitions
  - Subtask 2.5: Transformations
  - Subtask 2.6: Lazy Evaluation
  - Subtask 2.7: Actions
  - Subtask 2.8: DataFrames and SQL

[Link to Assignment Repository](https://github.com/yaz1997/Sparks-repo/tree/main/day1)




## Day 2

- *Task 1: Structured API Overview*
  - Subtask 1.1: DataFrames
  - Subtask 1.2: Schemas
  - Subtask 1.3: Overview of Structured Spark Types
  - Subtask 1.4: Columns
  - Subtask 1.5: Rows
  - Subtask 1.6: Spark Types
  - Subtask 1.7: Overview of Structured API Execution
    - Sub-subtask 1.7.1: Logical Planning
    - Sub-subtask 1.7.2: Physical Planning
    - Sub-subtask 1.7.3: Execution
  - Subtask 1.8: Conclusion

- *Task 2: Basic Structured Operations*
  - Subtask 2.1: Schemas
  - Subtask 2.2: Columns and Expressions
    - Sub-subtask 2.2.1: Columns
    - Sub-subtask 2.2.2: Expressions
  - Subtask 2.3: Records and Rows
    - Sub-subtask 2.3.1: Creating Rows
  - Subtask 2.4: DataFrame Transformations
    - Sub-subtask 2.4.1: Creating DataFrames
    - Sub-subtask 2.4.2: select and selectExpr
    - Sub-subtask 2.4.3: Converting to Spark Types (Literals)
    - Sub-subtask 2.4.4: Adding Columns
    - Sub-subtask 2.4.5: Renaming Columns
    - Sub-subtask 2.4.6: Reserved Characters and Keywords
    - Sub-subtask 2.4.7: Case Sensitivity
    - Sub-subtask 2.4.8: Removing Columns
    - Sub-subtask 2.4.9: Changing a Column’s Type (cast)
    - Sub-subtask 2.4.10: Filtering Rows
    - Sub-subtask 2.4.11: Getting Unique Rows
    - Sub-subtask 2.4.12: Random Samples
    - Sub-subtask 2.4.13: Random Splits
    - Sub-subtask 2.4.14: Concatenating and Appending Rows (Union)
    - Sub-subtask 2.4.15: Sorting Rows
    - Sub-subtask 2.4.16: Limit
    - Sub-subtask 2.4.17: Repartition and Coalesce
    - Sub-subtask 2.4.18: Collecting Rows to the Driver
  - Subtask 2.5: Conclusion
[Link to Assignment Repository](https://github.com/yaz1997/Sparks-repo/tree/main/day2)


## Day 3

- *Task 1: Working with Different Types of Data*
  - Subtask 1.1: Where to Look for APIs
  - Subtask 1.2: Converting to Spark Types
  - Subtask 1.3: Working with Booleans
  - Subtask 1.4: Working with Numbers
  - Subtask 1.5: Working with Strings
  - Subtask 1.6: Regular Expressions
  - Subtask 1.7: Working with Dates and Timestamps
  - Subtask 1.8: Working with Nulls in Data
  - Subtask 1.9: Coalesce
  - Subtask 1.10: ifnull, nullIf, nvl, and nvl2
  - Subtask 1.11: drop
  - Subtask 1.12: fill
  - Subtask 1.13: replace
  - Subtask 1.14: Ordering
  - Subtask 1.15: Working with Complex Types
    - Sub-subtask 1.15.1: Structs
    - Sub-subtask 1.15.2: Arrays
      - Sub-sub-subtask 1.15.2.1: split
      - Sub-sub-subtask 1.15.2.2: Array Length
      - Sub-sub-subtask 1.15.2.3: array_contains
      - Sub-sub-subtask 1.15.2.4: explode
    - Sub-subtask 1.15.3: Maps
    - Sub-subtask 1.15.4: Working with JSON
  - Subtask 1.16: User-Defined Functions
  - Subtask 1.17: Conclusion

[Link to Assignment Repository](https://github.com/yaz1997/Sparks-repo/tree/main/day3)

## Day 4

- *Task 1: Aggregations*
  - Subtask 1.1: Aggregation Functions
    - Sub-subtask 1.1.1: count
    - Sub-subtask 1.1.2: countDistinct
    - Sub-subtask 1.1.3: approx_count_distinct
    - Sub-subtask 1.1.4: first and last
    - Sub-subtask 1.1.5: min and max
    - Sub-subtask 1.1.6: sum
    - Sub-subtask 1.1.7: sumDistinct
    - Sub-subtask 1.1.8: avg
  - Subtask 1.2: Aggregating to Complex Types
  - Subtask 1.3: Grouping
  - Subtask 1.4: Window Functions
  - Subtask 1.5: Pivot
  - Subtask 1.6: User-Defined Aggregation Functions
  - Subtask 1.7: Conclusion

- *Task 2: Joins*
  - Subtask 2.1: Join Expressions
  - Subtask 2.2: Join Types
    - Sub-subtask 2.2.1: Inner Joins
    - Sub-subtask 2.2.2: Outer Joins
    - Sub-subtask 2.2.3: Left Outer Joins
    - Sub-subtask 2.2.4: Right Outer Joins
    - Sub-subtask 2.2.5: Left Semi Joins
    - Sub-subtask 2.2.6: Left Anti Joins
    - Sub-subtask 2.2.7: Natural Joins
    - Sub-subtask 2.2.8: Cross (Cartesian) Joins
  - Subtask 2.3: How Spark Performs Joins
  - Subtask 2.4: Conclusion

- *Task 3: Advanced Structured Operations*
  - Subtask 3.1: Advanced DataFrame Operations
  - Subtask 3.2: Window Functions
  - Subtask 3.3: User-Defined Functions (UDFs)
  - Subtask 3.4: Caching and Persistence
  - Subtask 3.5: Optimization and Performance Tuning
  - Subtask 3.6: Handling Missing Data
  - Subtask 3.7: Working with External Data Sources
  - Subtask 3.8: Structured Streaming Fundamentals
  - Subtask 3.9: Structured Streaming Operations
  - Subtask 3.10: Monitoring and Debugging Structured Streaming
  - Subtask 3.11: Conclusion

[Link to Assignment Repository](https://github.com/yaz1997/Sparks-repo/tree/main/day4)
