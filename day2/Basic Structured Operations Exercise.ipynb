{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Structured Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize PySpark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 08:19:41 WARN Utils: Your hostname, riyaz-Aspire-VX5-591G resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface enp3s0)\n",
      "23/08/31 08:19:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/31 08:19:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, lit , avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"day2\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = \"./occupation.csv\"  # Replace with the actual path\n",
    "occupation = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "occupation.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Selecting Specific Columns\n",
    "Problem: Select the \"user_id,\" \"age,\" and \"occupation\" columns from the occupation DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "occupation.createOrReplaceTempView(\"occupation_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|user_id|age|   occupation|\n",
      "+-------+---+-------------+\n",
      "|      1| 24|   technician|\n",
      "|      2| 53|        other|\n",
      "|      3| 23|       writer|\n",
      "|      4| 24|   technician|\n",
      "|      5| 33|        other|\n",
      "|      6| 42|    executive|\n",
      "|      7| 57|administrator|\n",
      "|      8| 36|administrator|\n",
      "|      9| 29|      student|\n",
      "|     10| 53|       lawyer|\n",
      "|     11| 39|        other|\n",
      "|     12| 28|        other|\n",
      "|     13| 47|     educator|\n",
      "|     14| 45|    scientist|\n",
      "|     15| 49|     educator|\n",
      "|     16| 21|entertainment|\n",
      "|     17| 30|   programmer|\n",
      "|     18| 35|        other|\n",
      "|     19| 40|    librarian|\n",
      "|     20| 42|    homemaker|\n",
      "+-------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SQL queries\n",
    "query = \"SELECT user_id, age, occupation FROM occupation_view\"\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Filtering Rows based on Condition\n",
    "Problem: Find the users who are older than 30 years from the occupation DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-------------+--------+\n",
      "|user_id|age|gender|   occupation|zip_code|\n",
      "+-------+---+------+-------------+--------+\n",
      "|      2| 53|     F|        other|   94043|\n",
      "|      5| 33|     F|        other|   15213|\n",
      "|      6| 42|     M|    executive|   98101|\n",
      "|      7| 57|     M|administrator|   91344|\n",
      "|      8| 36|     M|administrator|   05201|\n",
      "|     10| 53|     M|       lawyer|   90703|\n",
      "|     11| 39|     F|        other|   30329|\n",
      "|     13| 47|     M|     educator|   29206|\n",
      "|     14| 45|     M|    scientist|   55106|\n",
      "|     15| 49|     F|     educator|   97301|\n",
      "|     18| 35|     F|        other|   37212|\n",
      "|     19| 40|     M|    librarian|   02138|\n",
      "|     20| 42|     F|    homemaker|   95660|\n",
      "|     25| 39|     M|     engineer|   55107|\n",
      "|     26| 49|     M|     engineer|   21044|\n",
      "|     27| 40|     F|    librarian|   30030|\n",
      "|     28| 32|     M|       writer|   55369|\n",
      "|     29| 41|     M|   programmer|   94043|\n",
      "|     34| 38|     F|administrator|   42141|\n",
      "|     39| 41|     M|entertainment|   01040|\n",
      "+-------+---+------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \" SELECT * FROM occupation_view WHERE age > 30\"\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Counting and Grouping\n",
    "Problem: Count the number of users in each occupation from the occupation DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|   occupation|user_count|\n",
      "+-------------+----------+\n",
      "|    librarian|        51|\n",
      "|      retired|        14|\n",
      "|       lawyer|        12|\n",
      "|         none|         9|\n",
      "|       writer|        45|\n",
      "|   programmer|        66|\n",
      "|    marketing|        26|\n",
      "|        other|       105|\n",
      "|    executive|        32|\n",
      "|    scientist|        31|\n",
      "|      student|       196|\n",
      "|     salesman|        12|\n",
      "|       artist|        28|\n",
      "|   technician|        27|\n",
      "|administrator|        79|\n",
      "|     engineer|        67|\n",
      "|   healthcare|        16|\n",
      "|     educator|        95|\n",
      "|entertainment|        18|\n",
      "|    homemaker|         7|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT occupation, COUNT(*) AS user_count FROM occupation_view GROUP BY occupation\"\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Adding a New Column\n",
    "Problem: Add a new column \"age_group\" to the occupation DataFrame based on the age of the users. Divide users into age groups: \"18-25\", \"26-35\", \"36-50\", and \"51+\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-------------+--------+---------+\n",
      "|user_id|age|gender|   occupation|zip_code|age_group|\n",
      "+-------+---+------+-------------+--------+---------+\n",
      "|      1| 24|     M|   technician|   85711|    18-25|\n",
      "|      2| 53|     F|        other|   94043|      51+|\n",
      "|      3| 23|     M|       writer|   32067|    18-25|\n",
      "|      4| 24|     M|   technician|   43537|    18-25|\n",
      "|      5| 33|     F|        other|   15213|    26-35|\n",
      "|      6| 42|     M|    executive|   98101|    36-50|\n",
      "|      7| 57|     M|administrator|   91344|      51+|\n",
      "|      8| 36|     M|administrator|   05201|    36-50|\n",
      "|      9| 29|     M|      student|   01002|    26-35|\n",
      "|     10| 53|     M|       lawyer|   90703|      51+|\n",
      "|     11| 39|     F|        other|   30329|    36-50|\n",
      "|     12| 28|     F|        other|   06405|    26-35|\n",
      "|     13| 47|     M|     educator|   29206|    36-50|\n",
      "|     14| 45|     M|    scientist|   55106|    36-50|\n",
      "|     15| 49|     F|     educator|   97301|    36-50|\n",
      "|     16| 21|     M|entertainment|   10309|    18-25|\n",
      "|     17| 30|     M|   programmer|   06355|    26-35|\n",
      "|     18| 35|     F|        other|   37212|    26-35|\n",
      "|     19| 40|     M|    librarian|   02138|    36-50|\n",
      "|     20| 42|     F|    homemaker|   95660|    36-50|\n",
      "+-------+---+------+-------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "occupation_with_age_group = occupation.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 25), \"18-25\")\n",
    "    .when(col(\"age\").between(26, 35), \"26-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .otherwise(\"51+\")\n",
    ")\n",
    "occupation_with_age_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age_group` cannot be resolved. Did you mean one of the following? [`occupation_view`.`age`, `occupation_view`.`gender`, `occupation_view`.`user_id`, `occupation_view`.`occupation`, `occupation_view`.`zip_code`].; line 1 pos 30;\n'UpdateTable [assignment('age_group, CASE WHEN ((age#18 >= cast(18 as int)) AND (age#18 <= cast(25 as int))) THEN 18-25 WHEN ((age#18 >= cast(26 as int)) AND (age#18 <= cast(35 as int))) THEN 26-35 WHEN ((age#18 >= cast(36 as int)) AND (age#18 <= cast(50 as int))) THEN 36-50 ELSE 51+ END)]\n+- SubqueryAlias occupation_view\n   +- View (`occupation_view`, [user_id#17,age#18,gender#19,occupation#20,zip_code#21])\n      +- Relation [user_id#17,age#18,gender#19,occupation#20,zip_code#21] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUPDATE occupation_view\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m    SET age_group =\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    CASE\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m    END\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m result \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49msql(query)\n\u001b[1;32m     11\u001b[0m result\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[39m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age_group` cannot be resolved. Did you mean one of the following? [`occupation_view`.`age`, `occupation_view`.`gender`, `occupation_view`.`user_id`, `occupation_view`.`occupation`, `occupation_view`.`zip_code`].; line 1 pos 30;\n'UpdateTable [assignment('age_group, CASE WHEN ((age#18 >= cast(18 as int)) AND (age#18 <= cast(25 as int))) THEN 18-25 WHEN ((age#18 >= cast(26 as int)) AND (age#18 <= cast(35 as int))) THEN 26-35 WHEN ((age#18 >= cast(36 as int)) AND (age#18 <= cast(50 as int))) THEN 36-50 ELSE 51+ END)]\n+- SubqueryAlias occupation_view\n   +- View (`occupation_view`, [user_id#17,age#18,gender#19,occupation#20,zip_code#21])\n      +- Relation [user_id#17,age#18,gender#19,occupation#20,zip_code#21] csv\n"
     ]
    }
   ],
   "source": [
    "query = \"UPDATE occupation_view\\\n",
    "    SET age_group =\\\n",
    "    CASE\\\n",
    "        WHEN age BETWEEN 18 AND 25 THEN '18-25'\\\n",
    "        WHEN age BETWEEN 26 AND 35 THEN '26-35'\\\n",
    "        WHEN age BETWEEN 36 AND 50 THEN '36-50'\\\n",
    "        ELSE '51+'\\\n",
    "    END\\\n",
    "\"\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Creating DataFrames and Converting to Spark Types\n",
    "Problem: Given the provided code snippet, create a DataFrame df using the given data and schema. The schema includes columns for firstname, middlename, lastname, id, gender, and salary. After creating the DataFrame, print its schema and display its content without truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |null      |Smith   |36636|M     |3000.0|\n",
      "|Michael  |Rose      |null    |40288|M     |4000.0|\n",
      "|Robert   |null      |Williams|42114|M     |4000.0|\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000.0|\n",
      "|Jen      |Mary      |Brown   |null |F     |-1.0  |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining Schema for our dataframe\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "#Inserting data into the newly made schema\n",
    "\n",
    "data = [\n",
    "    (\"James\", None, \"Smith\", 36636, \"M\", 3000.0),\n",
    "    (\"Michael\", \"Rose\", None, 40288, \"M\", 4000.0),\n",
    "    (\"Robert\", None, \"Williams\", 42114, \"M\", 4000.0),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", 39192, \"F\", 4000.0),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", None, \"F\", -1.0)\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema and data\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "# Show the schema\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Display the content without truncation\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Adding and Renaming Columns\n",
    "Problem: Add a new column \"gender\" to the existing DataFrame and rename the \"Age\" column to \"Years\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- years: integer (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      "\n",
      "+-------+-----+-------+-------------+--------+\n",
      "|user_id|years|gender |occupation   |zip_code|\n",
      "+-------+-----+-------+-------------+--------+\n",
      "|1      |24   |Unknown|technician   |85711   |\n",
      "|2      |53   |Unknown|other        |94043   |\n",
      "|3      |23   |Unknown|writer       |32067   |\n",
      "|4      |24   |Unknown|technician   |43537   |\n",
      "|5      |33   |Unknown|other        |15213   |\n",
      "|6      |42   |Unknown|executive    |98101   |\n",
      "|7      |57   |Unknown|administrator|91344   |\n",
      "|8      |36   |Unknown|administrator|05201   |\n",
      "|9      |29   |Unknown|student      |01002   |\n",
      "|10     |53   |Unknown|lawyer       |90703   |\n",
      "|11     |39   |Unknown|other        |30329   |\n",
      "|12     |28   |Unknown|other        |06405   |\n",
      "|13     |47   |Unknown|educator     |29206   |\n",
      "|14     |45   |Unknown|scientist    |55106   |\n",
      "|15     |49   |Unknown|educator     |97301   |\n",
      "|16     |21   |Unknown|entertainment|10309   |\n",
      "|17     |30   |Unknown|programmer   |06355   |\n",
      "|18     |35   |Unknown|other        |37212   |\n",
      "|19     |40   |Unknown|librarian    |02138   |\n",
      "|20     |42   |Unknown|homemaker    |95660   |\n",
      "+-------+-----+-------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column \"gender\" with a constant value\n",
    "occupation_with_gender = occupation.withColumn(\"gender\", lit(\"Unknown\"))\n",
    "\n",
    "# Rename the \"age\" column to \"years\"\n",
    "occupation_final = occupation_with_gender.withColumnRenamed(\"age\", \"years\")\n",
    "\n",
    "# Show the schema and content\n",
    "occupation_final.printSchema()\n",
    "\n",
    "occupation_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: Filtering Rows and Sorting\n",
    "Problem: Filter out users who are younger than 30 years and sort the DataFrame by age in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-------------+--------+\n",
      "|user_id|years| gender|   occupation|zip_code|\n",
      "+-------+-----+-------+-------------+--------+\n",
      "|    481|   73|Unknown|      retired|   37771|\n",
      "|    767|   70|Unknown|     engineer|   00000|\n",
      "|    803|   70|Unknown|administrator|   78212|\n",
      "|    860|   70|Unknown|      retired|   48322|\n",
      "|    559|   69|Unknown|    executive|   10022|\n",
      "|    585|   69|Unknown|    librarian|   98501|\n",
      "|    349|   68|Unknown|      retired|   61455|\n",
      "|    573|   68|Unknown|      retired|   48911|\n",
      "|    211|   66|Unknown|     salesman|   32605|\n",
      "|    318|   65|Unknown|      retired|   06518|\n",
      "|    564|   65|Unknown|      retired|   94591|\n",
      "|    651|   65|Unknown|      retired|   02903|\n",
      "|    423|   64|Unknown|        other|   91606|\n",
      "|    845|   64|Unknown|       doctor|   97405|\n",
      "|    364|   63|Unknown|     engineer|   01810|\n",
      "|    777|   63|Unknown|   programmer|   01810|\n",
      "|    858|   63|Unknown|     educator|   09645|\n",
      "|    266|   62|Unknown|administrator|   78756|\n",
      "|    520|   62|Unknown|   healthcare|   12603|\n",
      "|    351|   61|Unknown|     educator|   49938|\n",
      "+-------+-----+-------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out users younger than 30 years\n",
    "filtered_occupation = occupation_final.filter(occupation.age >= 30)\n",
    "\n",
    "# Sort the DataFrame by age in descending order\n",
    "sorted_occupation = filtered_occupation.orderBy(occupation.age.desc())\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "sorted_occupation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: Repartitioning and Collecting Rows\n",
    "Problem: Repartition the DataFrame into 2 partitions without shuffling the data, then collect and display all rows in the driver and print number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(firstname='James', middlename=None, lastname='Smith', id=36636, gender='M', salary=3000.0)\n",
      "Row(firstname='Michael', middlename='Rose', lastname=None, id=40288, gender='M', salary=4000.0)\n",
      "Row(firstname='Robert', middlename=None, lastname='Williams', id=42114, gender='M', salary=4000.0)\n",
      "Row(firstname='Maria', middlename='Anne', lastname='Jones', id=39192, gender='F', salary=4000.0)\n",
      "Row(firstname='Jen', middlename='Mary', lastname='Brown', id=None, gender='F', salary=-1.0)\n"
     ]
    }
   ],
   "source": [
    "# Repartition the DataFrame into 2 partitions without shuffling\n",
    "repartitioned_df = df.coalesce(2)\n",
    "\n",
    "# Collect and display all rows in the driver\n",
    "all_rows = repartitioned_df.collect()\n",
    "for row in all_rows:\n",
    "    print(row)\n",
    "\n",
    "# Get the number of partitions\n",
    "num_partitions = repartitioned_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:\", num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional questions:\n",
    "\n",
    "Use both spark SQL and Pyspark to obtain answer wherever relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out rows where the age is greater than 30 and create a new DataFrame. Then, add a new column named \"is_elderly\" with a value of \"True\" for these rows and \"False\" otherwise.Rename the \"gender\" column to \"sex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_id: int, years: int, gender: string, occupation: string, zip_code: string]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age` cannot be resolved. Did you mean one of the following? [`occupation_view`.`gender`, `occupation_view`.`years`, `occupation_view`.`user_id`, `occupation_view`.`zip_code`, `occupation_view`.`occupation`].; line 1 pos 84;\n'Project ['user_id, 'age, 'gender AS sex#215, 'occupation, 'zip_code]\n+- 'Filter ('age > 30)\n   +- SubqueryAlias occupation_view\n      +- View (`occupation_view`, [user_id#17,years#108,gender#102,occupation#20,zip_code#21])\n         +- Project [user_id#17, years#108, gender#102, occupation#20, zip_code#21]\n            +- Sort [age#18 DESC NULLS LAST], true\n               +- Project [user_id#17, years#108, gender#102, occupation#20, zip_code#21, age#18]\n                  +- Filter (age#18 >= 30)\n                     +- Project [user_id#17, age#18 AS years#108, gender#102, occupation#20, zip_code#21, age#18]\n                        +- Project [user_id#17, age#18, Unknown AS gender#102, occupation#20, zip_code#21]\n                           +- Relation [user_id#17,age#18,gender#19,occupation#20,zip_code#21] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(sorted_occupation)\n\u001b[1;32m      7\u001b[0m \u001b[39m## Filter out rows where age is greater than 30\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m filtered_rows_sql \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT user_id, age, gender AS sex, occupation, zip_code FROM occupation_view WHERE age > 30\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Add a new column \"is_elderly\" with a value of True or False\u001b[39;00m\n\u001b[1;32m     11\u001b[0m filtered_rows_sql \u001b[39m=\u001b[39m filtered_rows_sql\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mis_elderly\u001b[39m\u001b[39m\"\u001b[39m, lit(\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[39m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age` cannot be resolved. Did you mean one of the following? [`occupation_view`.`gender`, `occupation_view`.`years`, `occupation_view`.`user_id`, `occupation_view`.`zip_code`, `occupation_view`.`occupation`].; line 1 pos 84;\n'Project ['user_id, 'age, 'gender AS sex#215, 'occupation, 'zip_code]\n+- 'Filter ('age > 30)\n   +- SubqueryAlias occupation_view\n      +- View (`occupation_view`, [user_id#17,years#108,gender#102,occupation#20,zip_code#21])\n         +- Project [user_id#17, years#108, gender#102, occupation#20, zip_code#21]\n            +- Sort [age#18 DESC NULLS LAST], true\n               +- Project [user_id#17, years#108, gender#102, occupation#20, zip_code#21, age#18]\n                  +- Filter (age#18 >= 30)\n                     +- Project [user_id#17, age#18 AS years#108, gender#102, occupation#20, zip_code#21, age#18]\n                        +- Project [user_id#17, age#18, Unknown AS gender#102, occupation#20, zip_code#21]\n                           +- Relation [user_id#17,age#18,gender#19,occupation#20,zip_code#21] csv\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "\n",
    "# Create a temporary view to use Spark SQL\n",
    "sorted_occupation.createOrReplaceTempView(\"occupation_view\")\n",
    "\n",
    "print(sorted_occupation)\n",
    "## Filter out rows where age is greater than 30\n",
    "filtered_rows_sql = spark.sql(\"SELECT user_id, age, gender AS sex, occupation, zip_code FROM occupation_view WHERE age > 30\")\n",
    "\n",
    "# Add a new column \"is_elderly\" with a value of True or False\n",
    "filtered_rows_sql = filtered_rows_sql.withColumn(\"is_elderly\", lit(True))\n",
    "\n",
    "# Show the resulting DataFrame without truncation\n",
    "filtered_rows_sql.show(100, False)\n",
    "\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "filtered_rows_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the average age of male and female users separately. Present the result in a new DataFrame with columns \"gender\" and \"avg_age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column named \"full_name\" to the dataset by concatenating the \"user_id\" and \"occupation\" columns. Then, rename the \"zip_code\" column to \"postal_code\" in the same DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out rows where occupation is 'technician', select only the \"user_id\" and \"age\" columns, and then add a new column \"age_diff\" that calculates the difference between the user's age and the average age in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the dataset into two DataFrames: one with male users and another with female users. Repartition both DataFrames to have 2 partitions each. Then, union these two DataFrames together and display the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and fill a new DataFrame named user_ratings with columns user_id and rating max 10 column. Both user_data and user_ratings share the user_id column. Combine these two DataFrames to create a new DataFrame that includes user information and their corresponding ratings. Make sure to keep only the users present in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
